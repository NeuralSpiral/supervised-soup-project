{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e7ca34",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Supervised-Soup/supervised-soup-project/blob/main/notebooks/colab_training_notebook_updated.ipynb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948889eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set cublas workspace config to make deterministic run possibl\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Code\n",
    "# @markdown This cell is to set up paths and dependencies and clone our repo.\n",
    "# This cell can be copy and pasted to the start of every new colab notebook.\n",
    "# Note on the latest changes: I have added a ColabCache folder on our shared drive, to save the dependency files.\n",
    "# This way the install of the requirements should be much faster, even across sessions.\n",
    "\n",
    "# mounting google drive to access the training data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# path for shared dataset\n",
    "import os\n",
    "os.environ[\"DATA_PATH\"] = '/content/drive/MyDrive/SupervisedSoupData/ImageNetSubset'\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "# verify path\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(\"Dataset found at:\", DATA_PATH)\n",
    "    print(\"Contents:\", os.listdir(DATA_PATH))\n",
    "else:\n",
    "    print(\"Dataset path not found. Please check if you have setup your Drive shortcut properly (see guide on confluence: https://stud-team-rn9zsvdn.atlassian.net/wiki/pages/resumedraft.action?draftId=6586396&draftShareId=6aea0c7c-2591-45b1-a0f8-f3db9e25e222).\")\n",
    "\n",
    "# integrating github by cloning our repo\n",
    "!git clone https://github.com/NeuralSpiral/supervised-soup-project.git\n",
    "%cd /content/supervised-soup-project\n",
    "\n",
    "# to install the dependencies\n",
    "# !pip install -r requirements.txt\n",
    "CACHE_PATH = \"/content/drive/MyDrive/SupervisedSoupData/ColabCache/pip\"\n",
    "\n",
    "!pip install --no-index --find-links={CACHE_PATH} -r requirements.txt\n",
    "!pip install -e .\n",
    "\n",
    "# run short import test\n",
    "!python tests/setup_test.py\n",
    "\n",
    "# add path to our src folder\n",
    "import sys\n",
    "sys.path.append('/content/supervised-soup-project/supervised_soup')\n",
    "\n",
    "# now we can import the code from our main folder if we need it for the notebook (e.g. dataloader, model), e.g.:\n",
    "# from supervised_soup import dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b63112",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title How to cache dependencies on drive for faster install time\n",
    "\n",
    "# Only run this cell if requirements.txt changes, refreshes cached wheels on Drive for faster installs\n",
    "\n",
    "%cd /content/supervised-soup-project\n",
    "\n",
    "# download and cache all wheel files\n",
    "!pip download -r requirements.txt -d \"/content/drive/MyDrive/SupervisedSoupData/ColabCache/pip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bb606",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the dataset from Drive to the Colab local VM for faster training\n",
    "# without this training was basically at CPU speed even with GPUs \n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DRIVE_DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "LOCAL_DATA_PATH = \"/content/data\"\n",
    "\n",
    "if not os.path.exists(LOCAL_DATA_PATH):\n",
    "    print(\"Copying dataset from Drive â†’ local VM...\")\n",
    "    shutil.copytree(DRIVE_DATA_PATH, LOCAL_DATA_PATH)\n",
    "else:\n",
    "    print(\"Local dataset already exists, skipping copy.\")\n",
    "\n",
    "# Override DATA_PATH for faster training\n",
    "os.environ[\"DATA_PATH\"] = LOCAL_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6acd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the dataset exists and show a sample image\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "train_dir = os.path.join(DATA_PATH, \"train\")\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    raise RuntimeError(\"Train directory not found. Check DATA_PATH.\")\n",
    "\n",
    "print(\"Sample classes:\", os.listdir(train_dir)[:5])\n",
    "\n",
    "sample_class = os.listdir(train_dir)[0]\n",
    "sample_image = os.listdir(os.path.join(train_dir, sample_class))[0]\n",
    "\n",
    "img = Image.open(os.path.join(train_dir, sample_class, sample_image))\n",
    "plt.imshow(img)\n",
    "plt.title(sample_class)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e09be1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available and print GPU info\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598a679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "# Cell defines all experiment-related configurations, easier reproducibility with mutiple experiments\n",
    "# Change the values here for your experiment run\n",
    "# current values are example for a baseline training run\n",
    "\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"experiment_name\": \"baseline_resnet18_frozen\",\n",
    "\n",
    "    # Training configs\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Model configuration\n",
    "    \"model_name\": \"resnet18\",\n",
    "    \"pretrained\": True,\n",
    "    \"freeze_layers\": True,\n",
    "\n",
    "    # Augmentation\n",
    "    \"with_augmentation\": False,\n",
    "\n",
    "    # Optimizer and hyperparameters\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    # Scheduler\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"min_lr\": 1e-6,\n",
    "\n",
    "    # wandb stuff\n",
    "    \"wandb_group\": \"baseline_frozen\",\n",
    "    \"wandb_name\": \"resnet18_seed42_pretrained_frozen_noAug\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a548a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Login to Weights & Biases for experiment tracking\n",
    "\n",
    "import wandb\n",
    "os.environ.pop(\"WANDB_ENTITY\", None)\n",
    "os.environ.pop(\"WANDB_PROJECT\", None)\n",
    "os.environ[\"WANDB_ENTITY\"] = \"neural-spi\"\n",
    "\n",
    "wandb.config.update(EXPERIMENT_CONFIG, allow_val_change=False)\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8282782",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title General training run cell\n",
    "# runs a full training experiment using the parameters from EXPERIMENT_CONFIG.\n",
    "\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=EXPERIMENT_CONFIG[\"epochs\"],\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    is_frozen=EXPERIMENT_CONFIG[\"is_frozen\"],\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=EXPERIMENT_CONFIG[\"wandb_group\"],\n",
    "    wandb_name=EXPERIMENT_CONFIG[\"wandb_name\"],\n",
    "    run_type=EXPERIMENT_CONFIG[\"experiment_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937b1cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Short test run (3 epochs)\n",
    "# Example run to test training works and metrics log to wandb\n",
    "# Run for debugging and validation before long runs\n",
    "\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=3,\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    is_frozen=EXPERIMENT_CONFIG[\"is_frozen\"],\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=f\"{EXPERIMENT_CONFIG['wandb_group']}_test\",\n",
    "    wandb_name=f\"{EXPERIMENT_CONFIG['wandb_name']}_3ep_test\",\n",
    "    run_type=f\"{EXPERIMENT_CONFIG['experiment_name']}_test\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bd579",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This was for debugging stuff\n",
    "\n",
    "# Print W&B run info, GPU memory usage, and sample local dataset folders for debugging\n",
    "print(\"W&B entity:\", wandb.run.entity)\n",
    "print(\"W&B project:\", wandb.run.project)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "print(\"Local dataset folders:\", os.listdir(LOCAL_DATA_PATH)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8003b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Resume training from the last saved checkpoint if it exists\n",
    "# Run this cell if a previous run was interrupted\n",
    "\n",
    "import os\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "LAST_CHECKPOINT_PATH = os.path.join(\n",
    "    \"/content/supervised-soup-project\",\n",
    "    \"wandb/latest-run/last_model.pt\",\n",
    ")\n",
    "\n",
    "if not os.path.exists(LAST_CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Checkpoint not found at {LAST_CHECKPOINT_PATH}. \"\n",
    "        \"Run a training cell first to generate a checkpoint.\"\n",
    "    )\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=EXPERIMENT_CONFIG[\"epochs\"],\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    is_frozen=EXPERIMENT_CONFIG[\"is_frozen\"],\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=EXPERIMENT_CONFIG[\"wandb_group\"],\n",
    "    wandb_name=f\"{EXPERIMENT_CONFIG['wandb_name']}_resume\",\n",
    "    run_type=f\"{EXPERIMENT_CONFIG['experiment_name']}_resume\",\n",
    "    last_checkpoint_path=LAST_CHECKPOINT_PATH,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
