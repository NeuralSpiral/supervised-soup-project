{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e7ca34",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Supervised-Soup/supervised-soup-project/blob/main/notebooks/colab_training_notebook_updated.ipynb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948889eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set cublas workspace config to make deterministic run possibl\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Code\n",
    "# @markdown This cell is to set up paths and dependencies and clone our repo.\n",
    "# This cell can be copy and pasted to the start of every new colab notebook.\n",
    "# Note on the latest changes: I have added a ColabCache folder on our shared drive, to save the dependency files.\n",
    "# This way the install of the requirements should be much faster, even across sessions.\n",
    "\n",
    "# mounting google drive to access the training data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# path for shared dataset\n",
    "import os\n",
    "os.environ[\"DATA_PATH\"] = '/content/drive/MyDrive/SupervisedSoupData/ImageNetSubset'\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "# verify path\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(\"Dataset found at:\", DATA_PATH)\n",
    "    print(\"Contents:\", os.listdir(DATA_PATH))\n",
    "else:\n",
    "    print(\"Dataset path not found. Please check if you have setup your Drive shortcut properly (see guide on confluence: https://stud-team-rn9zsvdn.atlassian.net/wiki/pages/resumedraft.action?draftId=6586396&draftShareId=6aea0c7c-2591-45b1-a0f8-f3db9e25e222).\")\n",
    "\n",
    "# integrating github by cloning our repo\n",
    "!git clone https://github.com/NeuralSpiral/supervised-soup-project.git\n",
    "%cd /content/supervised-soup-project\n",
    "\n",
    "# to install the dependencies\n",
    "# !pip install -r requirements.txt\n",
    "CACHE_PATH = \"/content/drive/MyDrive/SupervisedSoupData/ColabCache/pip\"\n",
    "\n",
    "!pip install --no-index --find-links={CACHE_PATH} -r requirements.txt\n",
    "!pip install -e .\n",
    "\n",
    "# run short import test\n",
    "!python tests/setup_test.py\n",
    "\n",
    "# add path to our src folder\n",
    "import sys\n",
    "sys.path.append('/content/supervised-soup-project/supervised_soup')\n",
    "\n",
    "# now we can import the code from our main folder if we need it for the notebook (e.g. dataloader, model), e.g.:\n",
    "# from supervised_soup import dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Dataset Selection\n",
    "\n",
    "# You only need to change this to \"cleaned\" for experiments on the other dataset\n",
    "# Options: \"original\" or \"cleaned\"\n",
    "DATASET_VERSION = \"original\"  \n",
    "\n",
    "# Base path where datasets are stored\n",
    "BASE_DATA_PATH = '/content/drive/MyDrive/SupervisedSoupData'\n",
    "\n",
    "# Set DATA_PATH environment variable based on selection\n",
    "if DATASET_VERSION == \"original\":\n",
    "    dataset_folder = \"ImageNetSubset\"\n",
    "elif DATASET_VERSION == \"cleaned\":\n",
    "    dataset_folder = \"ImageNetSubset_cleaned\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DATASET_VERSION={DATASET_VERSION}. Choose 'original' or 'cleaned'.\")\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_DATA_PATH, dataset_folder)\n",
    "os.environ[\"DATA_PATH\"] = DATA_PATH\n",
    "\n",
    "# Verify the dataset path exists\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"Dataset ({DATASET_VERSION}) found at: {DATA_PATH}\")\n",
    "    print(\"Contents:\", os.listdir(DATA_PATH)[:5])\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset path not found: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b63112",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title How to cache dependencies on drive for faster install time\n",
    "\n",
    "# Only run this cell if requirements.txt changes, refreshes cached wheels on Drive for faster installs\n",
    "\n",
    "%cd /content/supervised-soup-project\n",
    "\n",
    "# download and cache all wheel files\n",
    "!pip download -r requirements.txt -d \"/content/drive/MyDrive/SupervisedSoupData/ColabCache/pip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bb606",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the dataset from Drive to the Colab local VM for faster training\n",
    "# without this training was basically at CPU speed even with GPUs \n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DRIVE_DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "LOCAL_DATA_PATH = \"/content/data\"\n",
    "\n",
    "if not os.path.exists(LOCAL_DATA_PATH):\n",
    "    print(\"Copying dataset from Drive â†’ local VM...\")\n",
    "    shutil.copytree(DRIVE_DATA_PATH, LOCAL_DATA_PATH)\n",
    "else:\n",
    "    print(\"Local dataset already exists, skipping copy.\")\n",
    "\n",
    "# Override DATA_PATH for faster training\n",
    "os.environ[\"DATA_PATH\"] = LOCAL_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6acd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the dataset exists and show a sample image\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "train_dir = os.path.join(DATA_PATH, \"train\")\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    raise RuntimeError(\"Train directory not found. Check DATA_PATH.\")\n",
    "\n",
    "print(\"Sample classes:\", os.listdir(train_dir)[:5])\n",
    "\n",
    "sample_class = os.listdir(train_dir)[0]\n",
    "sample_image = os.listdir(os.path.join(train_dir, sample_class))[0]\n",
    "\n",
    "img = Image.open(os.path.join(train_dir, sample_class, sample_image))\n",
    "plt.imshow(img)\n",
    "plt.title(sample_class)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e09be1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available and print GPU info\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598a679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "# Cell defines all experiment-related configurations, easier reproducibility with mutiple experiments\n",
    "# Change the values here for your experiment run\n",
    "# current values are example for a baseline training run\n",
    "# For wandb naming conventions see the doc file on confluence\n",
    "\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"experiment_name\": \"baseline_resnet18_frozen\",\n",
    "\n",
    "    # Dataset stuff\n",
    "    \"dataset_version\": DATASET_VERSION, \n",
    "    \"data_path\": os.environ[\"DATA_PATH\"],\n",
    "    \n",
    "    # Training configs\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 64,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Model configuration\n",
    "    \"model_name\": \"resnet18\",\n",
    "    \"pretrained\": True,\n",
    "    \"freeze_layers\": True,\n",
    "    \"freeze_until\": None,\n",
    "\n",
    "    # Augmentation\n",
    "    \"with_augmentation\": False,\n",
    "\n",
    "    # Optimizer and hyperparameters\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    # Scheduler\n",
    "    \"scheduler\": \"cosine\",\n",
    "    \"min_lr\": 1e-6,\n",
    "\n",
    "\n",
    "    \"loss\": \"cross_entropy\",\n",
    "\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "\n",
    "    # wandb stuff\n",
    "    \"wandb_project\": \"x-AI-Proj-ImageClassification\",\n",
    "    \"wandb_group\": \"baseline_frozen\",\n",
    "    # wandb_name should be a unique, descripitve name for every individual run\n",
    "\n",
    "}\n",
    "\n",
    "# experiment and wandb names now auto-generated from configs (no accidental misnaming)\n",
    "\n",
    "# set a freee_tag based on configs\n",
    "if EXPERIMENT_CONFIG[\"freeze_layers\"]:\n",
    "    freeze_tag = \"frozen\"\n",
    "elif EXPERIMENT_CONFIG[\"freeze_until\"] is not None:\n",
    "    freeze_tag = f\"partial_{EXPERIMENT_CONFIG['freeze_until']}\"\n",
    "else:\n",
    "    freeze_tag = \"full_finetune\"\n",
    "\n",
    "# wandb_name\n",
    "EXPERIMENT_CONFIG[\"wandb_name\"] = (\n",
    "    f\"{EXPERIMENT_CONFIG['model_name']}_\"\n",
    "    f\"{freeze_tag}_\"\n",
    "    f\"aug{int(EXPERIMENT_CONFIG['with_augmentation'])}_\"\n",
    "    f\"seed{EXPERIMENT_CONFIG['seed']}_\"\n",
    "    f\"ds{EXPERIMENT_CONFIG['dataset_version']}\"\n",
    ")\n",
    "\n",
    "# experiment_name (less detailed)\n",
    "EXPERIMENT_CONFIG[\"experiment_name\"] = (\n",
    "    f\"{EXPERIMENT_CONFIG['model_name']}_\"\n",
    "    f\"ds{EXPERIMENT_CONFIG['dataset_version']}_\"\n",
    "    f\"aug{int(EXPERIMENT_CONFIG['with_augmentation'])}\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a548a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Login to Weights & Biases for experiment tracking with your API key\n",
    "\n",
    "import wandb\n",
    "os.environ.pop(\"WANDB_ENTITY\", None)\n",
    "os.environ.pop(\"WANDB_PROJECT\", None)\n",
    "os.environ[\"WANDB_ENTITY\"] = \"neural-spi-university\"\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8282782",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title General training run cell\n",
    "# runs a full training experiment using the parameters from EXPERIMENT_CONFIG.\n",
    "\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=EXPERIMENT_CONFIG[\"epochs\"],\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    model_name=EXPERIMENT_CONFIG.get(\"model_name\"),\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    freeze_layers=EXPERIMENT_CONFIG[\"freeze_layers\"],\n",
    "    freeze_until=EXPERIMENT_CONFIG.get(\"freeze_until\"),\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=EXPERIMENT_CONFIG[\"wandb_group\"],\n",
    "    wandb_name=EXPERIMENT_CONFIG[\"wandb_name\"],\n",
    "    run_type=EXPERIMENT_CONFIG[\"experiment_name\"],\n",
    "    experiment_config=EXPERIMENT_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937b1cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Short test run (3 epochs)\n",
    "# Example run to test training works and metrics log to wandb\n",
    "# Run for debugging and validation before long runs\n",
    "\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=3,\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    model_name=EXPERIMENT_CONFIG.get(\"model_name\"),\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    freeze_layers=EXPERIMENT_CONFIG[\"freeze_layers\"],\n",
    "    freeze_until=EXPERIMENT_CONFIG.get(\"freeze_until\"),\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=f\"{EXPERIMENT_CONFIG['wandb_group']}_test\",\n",
    "    wandb_name=f\"{EXPERIMENT_CONFIG['wandb_name']}_3ep_test\",\n",
    "    run_type=f\"{EXPERIMENT_CONFIG['experiment_name']}_test\",\n",
    "    experiment_config=EXPERIMENT_CONFIG,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bd579",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This was for debugging stuff\n",
    "\n",
    "# Print W&B run info, GPU memory usage, and sample local dataset folders for debugging\n",
    "print(\"W&B entity:\", wandb.run.entity)\n",
    "print(\"W&B project:\", wandb.run.project)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "print(\"Local dataset folders:\", os.listdir(LOCAL_DATA_PATH)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8003b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Resume training from the last saved checkpoint if it exists\n",
    "# Run this cell if a previous run was interrupted\n",
    "\n",
    "import os\n",
    "from supervised_soup.train import run_training\n",
    "\n",
    "model, history = run_training(\n",
    "    epochs=EXPERIMENT_CONFIG[\"epochs\"],\n",
    "    lr=EXPERIMENT_CONFIG[\"learning_rate\"],\n",
    "    with_augmentation=EXPERIMENT_CONFIG[\"with_augmentation\"],\n",
    "    pretrained=EXPERIMENT_CONFIG[\"pretrained\"],\n",
    "    freeze_layers=EXPERIMENT_CONFIG[\"freeze_layers\"],\n",
    "    seed=EXPERIMENT_CONFIG[\"seed\"],\n",
    "    wandb_group=EXPERIMENT_CONFIG[\"wandb_group\"],\n",
    "    wandb_name=f\"{EXPERIMENT_CONFIG['wandb_name']}_resume\",\n",
    "    run_type=f\"{EXPERIMENT_CONFIG['experiment_name']}_resume\",\n",
    "    resume=True,\n",
    "    experiment_config=EXPERIMENT_CONFIG,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c89cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# How to print run history to string\n",
    "# specify entity/project/run_id\n",
    "# e.g. run = api.run(\"neural-spi-university/x-AI-Proj-ImageClassification/f8wn0wc1\")\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "run = api.run(\"neural-spi-university/x-AI-Proj-ImageClassification/f8wn0wc1\")\n",
    "\n",
    "cols = [\n",
    "    \"epoch\",\n",
    "    \"train/loss\",\n",
    "    \"train/accuracy\",\n",
    "    \"val/loss\",\n",
    "    \"val/accuracy\",\n",
    "    \"val/f1_macro\",\n",
    "    \"val/roc_auc_macro\",\n",
    "]\n",
    "\n",
    "df = run.history(keys=cols)\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
